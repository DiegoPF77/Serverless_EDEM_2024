{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script: Dataflow Basics\n",
    "\n",
    "Description: Notebook where we will see the functioning of each transformation discussed during the theory.\n",
    "\n",
    "EDEM. Master Data Analytics<br>\n",
    "Professor: Javi Briones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCP Auth\n",
    "\n",
    "# Local\n",
    "!gcloud auth application-default login\n",
    "\n",
    "# Google Colab\n",
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements\n",
    "!pip install \"apache_beam[interactive]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python Libraries\n",
    "import logging\n",
    "import apache_beam as beam\n",
    "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
    "import apache_beam.runners.interactive.interactive_beam as ib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../00_DocAux/.images/Beam_Pipeline.png\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 01 Understanding basic concepts: PCollection, PTransform & Pipeline Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "\n",
    "    (p   \n",
    "        | \"Read Text from a File\" >> beam.io.ReadFromText('../00_DocAux/input_text.txt')\n",
    "        | \"Show content\" >> beam.Map(print))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 02 Understanding Core Transformations: DoFn & Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map\n",
    "def edem_map(element, num):\n",
    "    return element * num\n",
    "\n",
    "# DoFn\n",
    "class edemDoFn(beam.DoFn):\n",
    "\n",
    "    def __init__(self, num):\n",
    "        self.num_ = num\n",
    "\n",
    "    def process(self, element):\n",
    "        yield element * self.num_\n",
    "\n",
    "# Pipeline\n",
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "  data = (\n",
    "      p \n",
    "        | \"Create a PCollection\" >> beam.Create([1,2,3,4,5])\n",
    "        | \"Map\" >> beam.Map(edem_map, num=2)\n",
    "        | \"DoFn\" >> beam.ParDo(edemDoFn(4))\n",
    "        | \"Print\" >> beam.Map(print)\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PTransform\n",
    "class edem_PTransform(beam.PTransform):\n",
    "\n",
    "    # Map\n",
    "    def edem_map(element):\n",
    "        return element * 2\n",
    "\n",
    "    # DoFn\n",
    "    class edemDoFn(beam.DoFn):\n",
    "\n",
    "        def process(self, element, num):\n",
    "            yield element * num\n",
    "    \n",
    "    def expand(self,PColl):\n",
    "        \n",
    "        PColl_ = (PColl \n",
    "            | \"Map\" >> beam.Map(lambda x: x * 2)\n",
    "            | \"ParDo\" >> beam.ParDo(edemDoFn(), num=4)\n",
    "            | \"Print\" >> beam.Map(print))\n",
    "        \n",
    "        yield PColl_\n",
    "\n",
    "# Pipeline\n",
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "    data = (\n",
    "        p \n",
    "            | \"Create a PCollection\" >> beam.Create([1,2,3,4,5])\n",
    "    )\n",
    "    \n",
    "    data | edem_PTransform() | \"Print\" >> beam.Map(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 03 DoFn Lifecycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "class DoFnLifeCycle(beam.DoFn):\n",
    "\n",
    "  def now(self):\n",
    "    self._now = datetime.now()\n",
    "    return self._now\n",
    "\n",
    "  def __init__(self):\n",
    "    print(\"Constructor started at: %s\" % self.now())\n",
    "\n",
    "  def setup(self):\n",
    "    print(\"worker started at: %s\" % self.now())\n",
    "\n",
    "  def start_bundle(self):\n",
    "    print(\"bundle started at: %s\" % self.now())\n",
    "\n",
    "  def process(self, element):\n",
    "    words = element.split()\n",
    "    for word in words:\n",
    "      print(\"Processing element: %s\" % word)\n",
    "      yield word.upper()\n",
    "\n",
    "  def finish_bundle(self):\n",
    "    print(\"bundle finished at: %s\" % self.now())\n",
    "\n",
    "  def teardown(self):\n",
    "    print(\"worker finished at: %s\" % self.now())\n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "  input_data = (\n",
    "      p \n",
    "        | \"Reading the input file\" >> beam.io.ReadFromText('../00_DocAux/input_text.txt')\n",
    "        | \"DoFn Life Cycle\" >> beam.ParDo(DoFnLifeCycle())\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 04 Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GroupByKey\n",
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "\n",
    "    data = (p | \"PCollection\" >> beam.Create([('Spain', 'Valencia'), ('Spain','Barcelona'), ('France', 'Paris')]))\n",
    "\n",
    "    (data \n",
    "        | \"Combined\" >> beam.GroupByKey()\n",
    "        | \"Print\" >> beam.Map(print))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CoGroupByKey\n",
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "\n",
    "    p1 = p | \"PCollection 01\" >> beam.Create([('Spain', 'Valencia'), ('Spain','Barcelona'), ('France', 'Paris')])\n",
    "    p2 = p | \"PCollection 02\" >> beam.Create([('Spain', 'Madrid'), ('Spain','Alicante'), ('France', 'Lyon')])\n",
    "\n",
    "    data = ((p1,p2) | beam.CoGroupByKey())\n",
    "\n",
    "    data | \"Print\" >> beam.Map(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine\n",
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "\n",
    "    data = (p | \"PCollection\" >> beam.Create([('User1', 1), ('User2', 5), ('User1', 7)]))\n",
    "\n",
    "    (data \n",
    "        | \"Combined\" >> beam.CombinePerKey(sum)\n",
    "        | \"Print\" >> beam.Map(print))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten\n",
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "\n",
    "    p1 = p | \"PCollection 01\" >> beam.Create(['New York', 'Los Angeles', 'Miami', 'Chicago'])\n",
    "    p2 = p | \"Pcollection 02\" >> beam.Create(['Madrid', 'Barcelona', 'Valencia', 'Malaga'])\n",
    "    p3 = p | \"Pcollection 03\" >> beam.Create(['London','Manchester', 'Liverpool'])\n",
    "\n",
    "    merged = ((p1,p2,p3)| beam.Flatten())\n",
    "\n",
    "    merged | beam.Map(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition\n",
    "countries = ['Spain', 'USA', 'Switzerland']\n",
    "\n",
    "def partition_fn(country,num_countries):\n",
    "    return countries.index(country['country'])\n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "\n",
    "        p1,p2,p3 = (\n",
    "                p \n",
    "                | \"PCollection\" >> beam.Create([\n",
    "                        {'country': 'Spain', 'city': 'Valencia'},\n",
    "                        {'country': 'Spain', 'city': 'Barcelona'},\n",
    "                        {'country': 'USA', 'city': 'New York'},\n",
    "                        {'country': 'Switzerland', 'city': 'Zurich'},\n",
    "                        {'country': 'Switzerland', 'city': 'Geneva'}  \n",
    "                ])\n",
    "                | \"partition\" >> beam.Partition(partition_fn, len(countries))\n",
    "        )\n",
    "\n",
    "        p3 | \"PCollection for Spain\" >> beam.Map(print)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 05 Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PubSub\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "with beam.Pipeline(options=PipelineOptions(streaming=True)) as p:\n",
    "\n",
    "    data = (p | \"ReadFromPubSub\" >> beam.io.ReadFromPubSub(subscription=''))\n",
    "\n",
    "    data | beam.Map(print)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bb80d8843f3fcc2b8a9e4270eecba2ac31de31ca61bfe968e7b44b35850adf72"
  },
  "kernelspec": {
   "display_name": "Python 3.10.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
