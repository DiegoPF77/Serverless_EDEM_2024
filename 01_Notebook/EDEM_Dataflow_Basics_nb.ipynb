{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script: Dataflow Basics\n",
    "\n",
    "Description: Notebook where we will see the functioning of each transformation discussed during the theory.\n",
    "\n",
    "EDEM. Master Data Analytics<br>\n",
    "Professor: Javi Briones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python Libraries\n",
    "import logging\n",
    "import apache_beam as beam\n",
    "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
    "import apache_beam.runners.interactive.interactive_beam as ib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Beam Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../00_DocAux/.images/beam_pipeline.png\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Apache Beam is a unified programming model for parallel data processing, providing a set of transformations that enable efficient manipulation and processing of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline, PCollection & PTransform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **PCollection**: In Apache Beam, *PCollection* represents an immutable collection of data flowing through a processing pipeline.\n",
    "\n",
    "- **Pipeline**: A *Pipeline* in Apache Beam defines a data processing flow, specifying the sequence of transformations to be applied to the PCollections.\n",
    "\n",
    "- **PTransform**: *PTransform* is an abstraction in Apache Beam that encapsulates a data transformation operation. It defines how an input PCollection is transformed into an output PCollection within the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('En', 1)\n",
      "('un', 2)\n",
      "('lugar', 1)\n",
      "('de', 12)\n",
      "('la', 1)\n",
      "('Mancha,', 1)\n",
      "('cuyo', 1)\n",
      "('nombre', 1)\n",
      "('no', 2)\n",
      "('quiero', 1)\n",
      "('acordarme,', 1)\n",
      "('ha', 1)\n",
      "('mucho', 1)\n",
      "('tiempo', 1)\n",
      "('que', 2)\n",
      "('vivía', 1)\n",
      "('hidalgo', 1)\n",
      "('los', 5)\n",
      "('lanza', 1)\n",
      "('en', 1)\n",
      "('astillero,', 1)\n",
      "('adarga', 1)\n",
      "('antigua,', 1)\n",
      "('rocín', 1)\n",
      "('flaco', 1)\n",
      "('y', 2)\n",
      "('galgo', 1)\n",
      "('corredor.', 1)\n",
      "('Una', 1)\n",
      "('olla', 1)\n",
      "('algo', 1)\n",
      "('más', 3)\n",
      "('vaca', 1)\n",
      "('carnero,', 1)\n",
      "('salpicón', 1)\n",
      "('las', 3)\n",
      "('noches,', 1)\n",
      "('duelos', 1)\n",
      "('quebrantos', 1)\n",
      "('sábados,', 1)\n",
      "('lantejas', 1)\n",
      "('viernes,', 1)\n",
      "('algún', 1)\n",
      "('palomino', 1)\n",
      "('añadidura', 1)\n",
      "('domingos,', 1)\n",
      "('consumían', 1)\n",
      "('tres', 1)\n",
      "('partes', 1)\n",
      "('su', 2)\n",
      "('hacienda.', 1)\n",
      "('El', 1)\n",
      "('resto', 1)\n",
      "('della', 1)\n",
      "('concluían', 1)\n",
      "('sayo', 1)\n",
      "('velarte,', 1)\n",
      "('calzas', 1)\n",
      "('velludo', 1)\n",
      "('para', 1)\n",
      "('fiestas', 1)\n",
      "('con', 2)\n",
      "('sus', 1)\n",
      "('pantuflos', 1)\n",
      "('lo', 2)\n",
      "('mismo,', 1)\n",
      "('días', 1)\n",
      "('entre', 1)\n",
      "('semana', 1)\n",
      "('se', 1)\n",
      "('honraba', 1)\n",
      "('vellorí', 1)\n",
      "('fino.', 1)\n"
     ]
    }
   ],
   "source": [
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "\n",
    "    (p   \n",
    "        | \"Read Text from a File\" >> beam.io.ReadFromText('../00_DocAux/input_text.txt')\n",
    "        | \"FlatMap\" >> beam.FlatMap(lambda z: z.split())\n",
    "        | \"Map\" >> beam.Map(lambda x: (x,1))\n",
    "        | \"Combine\" >> beam.CombinePerKey(sum)\n",
    "        | \"Print\" >> beam.Map(print))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ParDo vs Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ParDo: Applies a function to each element in a data bundle, allowing for more complex and flexible operations than the Map transformation.\n",
    "\n",
    "DoFn: Defines a function that can be used in ParDo transformations to perform more advanced and customized operations on the elements of a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "16\n",
      "24\n",
      "32\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "# Map\n",
    "def edem_map(element, num):\n",
    "    return element * num\n",
    "\n",
    "# DoFn\n",
    "class edemDoFn(beam.DoFn):\n",
    "\n",
    "    def __init__(self, num):\n",
    "        self.num_ = num\n",
    "\n",
    "    def process(self, element):\n",
    "        yield element * self.num_\n",
    "\n",
    "# Pipeline\n",
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "  data = (\n",
    "      p \n",
    "        | \"Create a PCollection\" >> beam.Create([1,2,3,4,5])\n",
    "        | \"Map\" >> beam.Map(edem_map, num=2)\n",
    "        | \"DoFn\" >> beam.ParDo(edemDoFn(4))\n",
    "        | \"Print\" >> beam.Map(print)\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DoFn Life Cycle\n",
    "\n",
    "The life cycle of a **DoFn** in Apache Beam refers to the phases that an instance of the DoFn class goes through from its initialization to its completion within the context of a *ParDo* transformation. Below, I describe the main stages of the DoFn life cycle:\n",
    "\n",
    "1. **Initialization: Setup**:\n",
    "\n",
    "   - Goal: This phase occurs once for each instance of DoFn before the transformation is executed.\n",
    "   - Process: Initialization tasks, such as configuring resources and preparing data that will be used during execution, are performed here.\n",
    "\n",
    "2. **Processing Elements (Process)**:\n",
    "\n",
    "   - Goal: This phase is executed for each input element in out data bundle.\n",
    "   - Process: The core processing logic is implemented in the *process(self,element)* method of the DoFn class. This method is called for each element and defines how each input is processed.\n",
    "\n",
    "3. **Start bundle or finish buncle**:\n",
    "\n",
    "   - Goal: This phase occurs once before/after all elements in a bundle (a portion of data processed in parallel) are being or have been processed.\n",
    "   - Process: Clean-up tasks and resource finalization used during bundle processing are carried out here.\n",
    "\n",
    "4. **Teardown**: \n",
    "\n",
    "   - Goal: This phase occurs once after all elements in the dataset have been processed.\n",
    "   - Process: Final cleaning and resource release tasks are executed, ensuring that all closing operations are completed successfully.\n",
    "\n",
    "The life cycle of a **DoFn** provides structured control over the execution of processing logic in *ParDo* transformations. **Each instance of DoFn is created, initialized, processes logic for each element, and is closed in a controlled manner**. This approach ensures proper resource management and facilitates the implementation of custom and clean data processing operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructor started at: 2024-01-30 16:40:53.148358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x000002115D545A20> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x000002115D545B40> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x000002115D546050> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x000002115D5460E0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x000002115D546290> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x000002115D546320> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x000002115D546440> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x000002115D5464D0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x000002115D546560> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x000002115D5465F0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x000002115D546830> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function add_impulse_to_dangling_transforms at 0x000002115D546950> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x000002115D5467A0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x000002115D5468C0> ====================\n",
      "INFO:apache_beam.runners.worker.statecache:Creating state cache with size 104857600\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x0000021162BB7D30> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.00 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker started at: 2024-01-30 16:40:53.800535\n",
      "bundle started at: 2024-01-30 16:40:53.803185\n",
      "Processing element: EN\n",
      "Processing element: UN\n",
      "Processing element: LUGAR\n",
      "Processing element: DE\n",
      "Processing element: LA\n",
      "Processing element: MANCHA,\n",
      "Processing element: DE\n",
      "Processing element: CUYO\n",
      "Processing element: NOMBRE\n",
      "Processing element: NO\n",
      "Processing element: QUIERO\n",
      "Processing element: ACORDARME,\n",
      "Processing element: NO\n",
      "Processing element: HA\n",
      "Processing element: MUCHO\n",
      "Processing element: TIEMPO\n",
      "Processing element: QUE\n",
      "Processing element: VIVÍA\n",
      "Processing element: UN\n",
      "Processing element: HIDALGO\n",
      "Processing element: DE\n",
      "Processing element: LOS\n",
      "Processing element: DE\n",
      "Processing element: LANZA\n",
      "Processing element: EN\n",
      "Processing element: ASTILLERO,\n",
      "Processing element: ADARGA\n",
      "Processing element: ANTIGUA,\n",
      "Processing element: ROCÍN\n",
      "Processing element: FLACO\n",
      "Processing element: Y\n",
      "Processing element: GALGO\n",
      "Processing element: CORREDOR.\n",
      "Processing element: UNA\n",
      "Processing element: OLLA\n",
      "Processing element: DE\n",
      "Processing element: ALGO\n",
      "Processing element: MÁS\n",
      "Processing element: VACA\n",
      "Processing element: QUE\n",
      "Processing element: CARNERO,\n",
      "Processing element: SALPICÓN\n",
      "Processing element: LAS\n",
      "Processing element: MÁS\n",
      "Processing element: NOCHES,\n",
      "Processing element: DUELOS\n",
      "Processing element: Y\n",
      "Processing element: QUEBRANTOS\n",
      "Processing element: LOS\n",
      "Processing element: SÁBADOS,\n",
      "Processing element: LANTEJAS\n",
      "Processing element: LOS\n",
      "Processing element: VIERNES,\n",
      "Processing element: ALGÚN\n",
      "Processing element: PALOMINO\n",
      "Processing element: DE\n",
      "Processing element: AÑADIDURA\n",
      "Processing element: LOS\n",
      "Processing element: DOMINGOS,\n",
      "Processing element: CONSUMÍAN\n",
      "Processing element: LAS\n",
      "Processing element: TRES\n",
      "Processing element: PARTES\n",
      "Processing element: DE\n",
      "Processing element: SU\n",
      "Processing element: HACIENDA.\n",
      "Processing element: EL\n",
      "Processing element: RESTO\n",
      "Processing element: DELLA\n",
      "Processing element: CONCLUÍAN\n",
      "Processing element: SAYO\n",
      "Processing element: DE\n",
      "Processing element: VELARTE,\n",
      "Processing element: CALZAS\n",
      "Processing element: DE\n",
      "Processing element: VELLUDO\n",
      "Processing element: PARA\n",
      "Processing element: LAS\n",
      "Processing element: FIESTAS\n",
      "Processing element: CON\n",
      "Processing element: SUS\n",
      "Processing element: PANTUFLOS\n",
      "Processing element: DE\n",
      "Processing element: LO\n",
      "Processing element: MISMO,\n",
      "Processing element: LOS\n",
      "Processing element: DÍAS\n",
      "Processing element: DE\n",
      "Processing element: ENTRE\n",
      "Processing element: SEMANA\n",
      "Processing element: SE\n",
      "Processing element: HONRABA\n",
      "Processing element: CON\n",
      "Processing element: SU\n",
      "Processing element: VELLORÍ\n",
      "Processing element: DE\n",
      "Processing element: LO\n",
      "Processing element: MÁS\n",
      "Processing element: FINO.\n",
      "bundle finished at: 2024-01-30 16:40:53.810144\n",
      "worker finished at: 2024-01-30 16:40:53.872584\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "class DoFnLifeCycle(beam.DoFn):\n",
    "\n",
    "  def teardown(self):\n",
    "    print(\"worker finished at: %s\" % self.now())\n",
    "\n",
    "  def now(self):\n",
    "    self._now = datetime.now()\n",
    "    return self._now\n",
    "\n",
    "  def __init__(self):\n",
    "    print(\"Constructor started at: %s\" % self.now())\n",
    "\n",
    "  def setup(self):\n",
    "    print(\"worker started at: %s\" % self.now())\n",
    "\n",
    "  def start_bundle(self):\n",
    "    print(\"bundle started at: %s\" % self.now())\n",
    "\n",
    "  def process(self, element):\n",
    "    words = element.split()\n",
    "    for word in words:\n",
    "      print(\"Processing element: %s\" % word.upper())\n",
    "      yield word.upper()\n",
    "\n",
    "  def finish_bundle(self):\n",
    "    print(\"bundle finished at: %s\" % self.now())\n",
    "\n",
    "  \n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "  input_data = (\n",
    "      p \n",
    "        | \"Reading the input file\" >> beam.io.ReadFromText('../00_DocAux/input_text.txt')\n",
    "        | \"DoFn Life Cycle\" >> beam.ParDo(DoFnLifeCycle())\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GroupByKey\n",
    "\n",
    "It Groups the elements of a data bundle according to a common key, generating a set where the keys are unique, and the values are lists of elements associated with each key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "\n",
    "    data = (p | \"PCollection\" >> beam.Create([('Spain', 'Valencia'), ('Spain','Barcelona'), ('France', 'Paris')]))\n",
    "\n",
    "    (data \n",
    "        | \"Combined\" >> beam.GroupByKey()\n",
    "        | \"Print\" >> beam.Map(print))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CoGroupByKey\n",
    "\n",
    "It merges two PCollections by key, generating pairs of keys and lists of associated elements from both data bundles. Used to perform operations involving data from two different sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "\n",
    "    p1 = p | \"PCollection 01\" >> beam.Create([('Spain', 'Valencia'), ('Spain','Barcelona'), ('France', 'Paris')])\n",
    "    p2 = p | \"PCollection 02\" >> beam.Create([('Spain', 'Madrid'), ('Spain','Alicante'), ('France', 'Lyon')])\n",
    "\n",
    "    data = ((p1,p2) | beam.CoGroupByKey())\n",
    "\n",
    "    data | \"Print\" >> beam.Map(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Flatten\n",
    "\n",
    "It combines multiple PCollections into a single collection, flattening the nested structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "\n",
    "    p1 = p | \"PCollection 01\" >> beam.Create(['New York', 'Los Angeles', 'Miami', 'Chicago'])\n",
    "    p2 = p | \"Pcollection 02\" >> beam.Create(['Madrid', 'Barcelona', 'Valencia', 'Malaga'])\n",
    "    p3 = p | \"Pcollection 03\" >> beam.Create(['London','Manchester', 'Liverpool'])\n",
    "\n",
    "    merged = ((p1,p2,p3)| beam.Flatten())\n",
    "\n",
    "    merged | beam.Map(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Partition\n",
    "\n",
    "it splits a PCollection into different partitions based on certain criteria, enabling parallel and distributed processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = ['Spain', 'USA', 'Switzerland']\n",
    "\n",
    "def partition_fn(country,num_countries):\n",
    "    return countries.index(country['country'])\n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "\n",
    "        p1,p2,p3 = (\n",
    "                p \n",
    "                | \"PCollection\" >> beam.Create([\n",
    "                        {'country': 'Spain', 'city': 'Valencia'},\n",
    "                        {'country': 'Spain', 'city': 'Barcelona'},\n",
    "                        {'country': 'USA', 'city': 'New York'},\n",
    "                        {'country': 'Switzerland', 'city': 'Zurich'},\n",
    "                        {'country': 'Switzerland', 'city': 'Geneva'}  \n",
    "                ])\n",
    "                | \"partition\" >> beam.Partition(partition_fn, len(countries))\n",
    "        )\n",
    "\n",
    "        p3 | \"PCollection for Switzerland\" >> beam.Map(print)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Combine\n",
    "\n",
    "It combines values associated with the same key using a specific combining function, useful for performing key-based aggregate operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "\n",
    "    data = (p | \"PCollection\" >> beam.Create([('User1', 1), ('User2', 5), ('User1', 7)]))\n",
    "\n",
    "    (data \n",
    "        | \"Combined\" >> beam.CombinePerKey(sum)\n",
    "        | \"Print\" >> beam.Map(print))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### gcloud commnads:\n",
    "\n",
    "- PubSub Topics\n",
    "\n",
    "```\n",
    "gcloud pubsub topics create <TOPIC_NAME>\n",
    "```\n",
    "\n",
    "- PubSub Subscriptions\n",
    "\n",
    "```\n",
    "gcloud pubsub subscriptions create <SUBSCRIPTION_NAME> --topic=<TOPIC_NAME>\n",
    "```\n",
    "\n",
    "- Google Cloud Storage Bucket  \n",
    "\n",
    "```\n",
    "gcloud storage mb gs://<BUCKET_NAME> --location=<REGION_ID>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "project_id = \"active-road-412714\"\n",
    "subscription_name = \"entorno1-sub\"\n",
    "bq_dataset = \"entorno1\"\n",
    "bq_table = \"entorno1\"\n",
    "bucket_name = \"entorno1dpf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PubSub - Dataflow - BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:The process started\n",
      "INFO:root:Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "INFO:apache_beam.runners.direct.direct_runner:Running pipeline with DirectRunner.\n",
      "INFO:root:New PubSub Message: {\"nombre\" : \"diego\"}\n",
      "INFO:apache_beam.internal.metrics.metric:[Locally aggregated metrics since 2024-01-30 15:20:19.004000]\n",
      "MetricName(namespace=apache_beam.io.gcp.bigquery_tools.BigQueryWrapper, name=latency_histogram_ms): HistogramData(Total count: 1, P99: 840, P90: 838, P50: 830)\n",
      "INFO:root:New PubSub Message: {\"nombre\" : \"balma\"}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe process started\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Run Process\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[28], line 18\u001b[0m, in \u001b[0;36mrun\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m():\n\u001b[1;32m---> 18\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m beam\u001b[38;5;241m.\u001b[39mPipeline(options\u001b[38;5;241m=\u001b[39mPipelineOptions(streaming\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, save_main_session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)) \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[0;32m     19\u001b[0m         (\n\u001b[0;32m     20\u001b[0m             p \n\u001b[0;32m     21\u001b[0m             \u001b[38;5;241m|\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReadFromPubSub\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m>>\u001b[39m beam\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mReadFromPubSub(subscription\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprojects/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproject_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/subscriptions/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubscription_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m             )\n\u001b[0;32m     29\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\diego\\Documents\\GitHub\\Serverless_EDEM_2024\\entorno1\\lib\\site-packages\\apache_beam\\pipeline.py:601\u001b[0m, in \u001b[0;36mPipeline.__exit__\u001b[1;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[0;32m    599\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exc_type:\n\u001b[0;32m    600\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m--> 601\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    603\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extra_context\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__exit__\u001b[39m(exc_type, exc_val, exc_tb)\n",
      "File \u001b[1;32mc:\\Users\\diego\\Documents\\GitHub\\Serverless_EDEM_2024\\entorno1\\lib\\site-packages\\apache_beam\\runners\\direct\\direct_runner.py:585\u001b[0m, in \u001b[0;36mDirectPipelineResult.wait_until_finish\u001b[1;34m(self, duration)\u001b[0m\n\u001b[0;32m    582\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    583\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDirectRunner does not support duration argument.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 585\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawait_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    586\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m=\u001b[39m PipelineState\u001b[38;5;241m.\u001b[39mDONE\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\diego\\Documents\\GitHub\\Serverless_EDEM_2024\\entorno1\\lib\\site-packages\\apache_beam\\runners\\direct\\executor.py:432\u001b[0m, in \u001b[0;36mExecutor.await_completion\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mawait_completion\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 432\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawait_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\diego\\Documents\\GitHub\\Serverless_EDEM_2024\\entorno1\\lib\\site-packages\\apache_beam\\runners\\direct\\executor.py:477\u001b[0m, in \u001b[0;36m_ExecutorServiceParallelExecutor.await_completion\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mawait_completion\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 477\u001b[0m   update \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisible_updates\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    478\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update\u001b[38;5;241m.\u001b[39mexception:\n",
      "File \u001b[1;32mc:\\Users\\diego\\Documents\\GitHub\\Serverless_EDEM_2024\\entorno1\\lib\\site-packages\\apache_beam\\runners\\direct\\executor.py:551\u001b[0m, in \u001b[0;36m_ExecutorServiceParallelExecutor._TypedUpdateQueue.take\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    550\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 551\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_queue\u001b[38;5;241m.\u001b[39mtask_done()\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m item\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "import json\n",
    "import logging\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "import sys\n",
    "\n",
    "sys.argv = ['--flexrs_goal', 'SPEED_OPTIMIZED']\n",
    "\n",
    "def decode_message(msg):\n",
    "\n",
    "    output = msg.decode('utf-8')\n",
    "\n",
    "    logging.info(\"New PubSub Message: %s\", output)\n",
    "\n",
    "    return json.loads(output)\n",
    "\n",
    "def run():\n",
    "    with beam.Pipeline(options=PipelineOptions(streaming=True, save_main_session=True)) as p:\n",
    "        (\n",
    "            p \n",
    "            | \"ReadFromPubSub\" >> beam.io.ReadFromPubSub(subscription=f'projects/{project_id}/subscriptions/{subscription_name}')\n",
    "            | \"decode msg\" >> beam.Map(decode_message)\n",
    "            | \"Write to BigQuery\" >> beam.io.WriteToBigQuery(\n",
    "                table = f\"{project_id}:{bq_dataset}.{bq_table}\", # Required Format: PROJECT_ID:DATASET.TABLE\n",
    "                schema='nombre:STRING', # Required Format: field:TYPE\n",
    "                create_disposition=beam.io.BigQueryDisposition.CREATE_NEVER,\n",
    "                write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
    "            )\n",
    "        )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Set Logs\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "    logging.info(\"The process started\")\n",
    "    \n",
    "    # Run Process\n",
    "    run()\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataflow (Google Cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:The process started\n",
      "INFO:root:Using provided Python SDK container image: gcr.io/cloud-dataflow/v1beta3/beam_python3.10_sdk:2.51.0\n",
      "INFO:root:Python SDK container image set to \"gcr.io/cloud-dataflow/v1beta3/beam_python3.10_sdk:2.51.0\" for Docker environment\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://entorno1dpf/staging/beamapp-diego-0130151406-463804-3z95y93g.1706627646.463804/pipeline.pb...\n",
      "INFO:oauth2client.transport:Attempting refresh to obtain initial access_token\n",
      "INFO:oauth2client.client:Refreshing access_token\n",
      "INFO:oauth2client.transport:Attempting refresh to obtain initial access_token\n",
      "INFO:oauth2client.client:Refreshing access_token\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://entorno1dpf/staging/beamapp-diego-0130151406-463804-3z95y93g.1706627646.463804/pipeline.pb in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " clientRequestId: '20240130151406463804-3594'\n",
      " createTime: '2024-01-30T15:14:09.054006Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2024-01-30_07_14_07-6903491149117288438'\n",
      " location: 'europe-west4'\n",
      " name: 'beamapp-diego-0130151406-463804-3z95y93g'\n",
      " projectId: 'active-road-412714'\n",
      " stageStates: []\n",
      " startTime: '2024-01-30T15:14:09.054006Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_STREAMING, 2)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2024-01-30_07_14_07-6903491149117288438]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Submitted job: 2024-01-30_07_14_07-6903491149117288438\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobs/europe-west4/2024-01-30_07_14_07-6903491149117288438?project=active-road-412714\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2024-01-30_07_14_07-6903491149117288438 is in state JOB_STATE_PENDING\n",
      "WARNING:apache_beam.runners.dataflow.dataflow_runner:2024-01-30T15:14:10.830Z: JOB_MESSAGE_WARNING: Autoscaling is enabled for Dataflow Streaming Engine. Workers will scale between 1 and 100 unless maxNumWorkers is specified.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-01-30T15:14:17.478Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-2 in europe-west4-b.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-01-30T15:14:20.266Z: JOB_MESSAGE_BASIC: Running job using Streaming Engine\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-01-30T15:14:20.571Z: JOB_MESSAGE_BASIC: Starting 1 workers in europe-west4-b...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2024-01-30_07_14_07-6903491149117288438 is in state JOB_STATE_RUNNING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-01-30T15:15:56.030Z: JOB_MESSAGE_BASIC: All workers have finished the startup processes and began to receive work requests.\n",
      "WARNING:apache_beam.runners.dataflow.dataflow_runner:2024-01-30T15:16:15.141Z: JOB_MESSAGE_WARNING: The worker utilization hint isn't being used by the autoscaler for the following reason: flag enabling worker utilization hint is not turned on. To use the utilization hint for autoscaling, set a value between {lower_bound_limit} and 0.9.\n",
      "INFO:root:New PubSub Message: {\"nombre\" : \"diegop\"}\n",
      "INFO:root:New PubSub Message: {\"nombre\" : \"diego\"}\n",
      "INFO:apache_beam.internal.metrics.metric:[Locally aggregated metrics since 2024-01-30 16:12:38.249000]\n",
      "MetricName(namespace=apache_beam.io.gcp.bigquery_tools.BigQueryWrapper, name=latency_histogram_ms): HistogramData(Total count: 2, P99: 540, P90: 536, P50: 420)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2024-01-30_07_14_07-6903491149117288438 is in state JOB_STATE_CANCELLING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-01-30T15:29:15.427Z: JOB_MESSAGE_BASIC: Cancel request is committed for workflow job: 2024-01-30_07_14_07-6903491149117288438.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-01-30T15:29:15.528Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-01-30T15:29:15.581Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2024-01-30T15:29:50.876Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Job did not reach to a terminal state after waiting indefinitely. Console URL: https://console.cloud.google.com/dataflow/jobs/<RegionId>/2024-01-30_07_14_07-6903491149117288438?project=<ProjectId>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe process started\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Run Process\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[37], line 17\u001b[0m, in \u001b[0;36mrun\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m():\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m beam\u001b[38;5;241m.\u001b[39mPipeline(options\u001b[38;5;241m=\u001b[39mPipelineOptions(\n\u001b[0;32m     18\u001b[0m         streaming\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;66;03m# save_main_session=True\u001b[39;00m\n\u001b[0;32m     20\u001b[0m         project\u001b[38;5;241m=\u001b[39mproject_id,\n\u001b[0;32m     21\u001b[0m         runner\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataflowRunner\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     22\u001b[0m         temp_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgs://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbucket_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/tmp\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     23\u001b[0m         staging_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgs://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbucket_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/staging\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     24\u001b[0m         region\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meurope-west4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     25\u001b[0m     )) \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[0;32m     26\u001b[0m         (\n\u001b[0;32m     27\u001b[0m             p \n\u001b[0;32m     28\u001b[0m             \u001b[38;5;241m|\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReadFromPubSub\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m>>\u001b[39m beam\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mReadFromPubSub(subscription\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprojects/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproject_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/subscriptions/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubscription_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m             )\n\u001b[0;32m     36\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\diego\\Documents\\GitHub\\Serverless_EDEM_2024\\entorno1\\lib\\site-packages\\apache_beam\\pipeline.py:601\u001b[0m, in \u001b[0;36mPipeline.__exit__\u001b[1;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[0;32m    599\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exc_type:\n\u001b[0;32m    600\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m--> 601\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    603\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extra_context\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__exit__\u001b[39m(exc_type, exc_val, exc_tb)\n",
      "File \u001b[1;32mc:\\Users\\diego\\Documents\\GitHub\\Serverless_EDEM_2024\\entorno1\\lib\\site-packages\\apache_beam\\runners\\dataflow\\dataflow_runner.py:759\u001b[0m, in \u001b[0;36mDataflowPipelineResult.wait_until_finish\u001b[1;34m(self, duration)\u001b[0m\n\u001b[0;32m    756\u001b[0m \u001b[38;5;66;03m# TODO: Merge the termination code in poll_for_job_completion and\u001b[39;00m\n\u001b[0;32m    757\u001b[0m \u001b[38;5;66;03m# is_in_terminal_state.\u001b[39;00m\n\u001b[0;32m    758\u001b[0m terminated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_in_terminal_state()\n\u001b[1;32m--> 759\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m duration \u001b[38;5;129;01mor\u001b[39;00m terminated, (\n\u001b[0;32m    760\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJob did not reach to a terminal state after waiting indefinitely. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    761\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(consoleUrl))\n\u001b[0;32m    763\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m!=\u001b[39m PipelineState\u001b[38;5;241m.\u001b[39mDONE:\n\u001b[0;32m    764\u001b[0m   \u001b[38;5;66;03m# TODO(BEAM-1290): Consider converting this to an error log based on\u001b[39;00m\n\u001b[0;32m    765\u001b[0m   \u001b[38;5;66;03m# theresolution of the issue.\u001b[39;00m\n\u001b[0;32m    766\u001b[0m   _LOGGER\u001b[38;5;241m.\u001b[39merror(consoleUrl)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Job did not reach to a terminal state after waiting indefinitely. Console URL: https://console.cloud.google.com/dataflow/jobs/<RegionId>/2024-01-30_07_14_07-6903491149117288438?project=<ProjectId>"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "import json\n",
    "import logging\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "#bucket_name = \"entorno1dpf\"\n",
    "\n",
    "def decode_message(msg):\n",
    "\n",
    "    output = msg.decode('utf-8')\n",
    "\n",
    "    logging.info(\"New PubSub Message: %s\", output)\n",
    "\n",
    "    return json.loads(output)\n",
    "\n",
    "def run():\n",
    "    with beam.Pipeline(options=PipelineOptions(\n",
    "        streaming=True,\n",
    "        # save_main_session=True\n",
    "        project=project_id,\n",
    "        runner=\"DataflowRunner\",\n",
    "        temp_location=f\"gs://{bucket_name}/tmp\",\n",
    "        staging_location=f\"gs://{bucket_name}/staging\",\n",
    "        region=\"europe-west4\"\n",
    "    )) as p:\n",
    "        (\n",
    "            p \n",
    "            | \"ReadFromPubSub\" >> beam.io.ReadFromPubSub(subscription=f'projects/{project_id}/subscriptions/{subscription_name}')\n",
    "            | \"decode msg\" >> beam.Map(decode_message)\n",
    "            | \"Write to BigQuery\" >> beam.io.WriteToBigQuery(\n",
    "                table = f\"{project_id}:{bq_dataset}.{bq_table}\",\n",
    "                schema='nombre:STRING',\n",
    "                create_disposition=beam.io.BigQueryDisposition.CREATE_NEVER,\n",
    "                write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
    "            )\n",
    "        )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Set Logs\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "    logging.info(\"The process started\")\n",
    "    \n",
    "    # Run Process\n",
    "    run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Window in Apache Beam:**\n",
    "A window in Apache Beam defines a time frame for **organizing and grouping data elements** during processing, enabling time-based specific operations.\n",
    "\n",
    "**Types of Windows:**\n",
    "\n",
    "- **Fixed Window:** Groups elements into fixed time intervals, dividing the PCollection into time-based windows.\n",
    "\n",
    "- **Sliding Window:** Allows overlapping windows, specified by a size and a stride, making it easy to analyze data in continuous intervals over time.\n",
    "\n",
    "- **Session Window:** Groups data elements that share a contiguous temporal relationship, where continuity is defined by the **inactivity gap between elements**. This dynamic window is formed based on the inactivity time between events, allowing the capture of logical sessions in data streams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fixed Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../00_DocAux/.images/fixed_window.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import json\n",
    "import logging\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "def decode_message(msg):\n",
    "\n",
    "    output = msg.decode('utf-8')\n",
    "\n",
    "    logging.info(\"New PubSub Message: %s\", output)\n",
    "\n",
    "    return json.loads(output)['temp']\n",
    "\n",
    "class OutputDoFn(beam.DoFn):\n",
    "\n",
    "    def process(self, element):\n",
    "        yield element\n",
    "\n",
    "def run():\n",
    "    with beam.Pipeline(options=PipelineOptions(streaming=True, save_main_session=True)) as p:\n",
    "        (\n",
    "            p \n",
    "            | \"ReadFromPubSub\" >> beam.io.ReadFromPubSub(subscription=f'projects/{project_id}/subscriptions/{subscription_name}')\n",
    "            | \"Decode msg\" >> beam.Map(decode_message)\n",
    "            | \"Fixed Window\" >> beam.WindowInto(beam.window.FixedWindows(10))\n",
    "            | \"Combine\" >> beam.CombineGlobally(sum).without_defaults()\n",
    "            | \"Print\" >> beam.Map(print)\n",
    "        )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Set Logs\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "    logging.info(\"The process started\")\n",
    "    \n",
    "    # Run Process\n",
    "    run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sliding windows "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../00_DocAux/.images/sliding_window.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import json\n",
    "import logging\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "def decode_message(msg):\n",
    "\n",
    "    output = msg.decode('utf-8')\n",
    "\n",
    "    logging.info(\"New PubSub Message: %s\", output)\n",
    "\n",
    "    return output['temp']\n",
    "\n",
    "class OutputDoFn(beam.DoFn):\n",
    "\n",
    "    def process(self, element):\n",
    "        yield element\n",
    "\n",
    "def run():\n",
    "    with beam.Pipeline(options=PipelineOptions(streaming=True, save_main_session=True)) as p:\n",
    "        (\n",
    "            p \n",
    "            | \"ReadFromPubSub\" >> beam.io.ReadFromPubSub(subscription=f'projects/{project_id}/subscriptions/{subscription_name}')\n",
    "            | \"Decode msg\" >> beam.Map(decode_message)\n",
    "            | \"Sliding Window\" >> beam.WindowInto(beam.window.SlidingWindows(size=60, period=20))\n",
    "            | \"Combine\" >> beam.CombinePerKey(sum)\n",
    "            | \"Print\" >> beam.Map(print)\n",
    "        )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Set Logs\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "    logging.info(\"The process started\")\n",
    "    \n",
    "    # Run Process\n",
    "    run()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bb80d8843f3fcc2b8a9e4270eecba2ac31de31ca61bfe968e7b44b35850adf72"
  },
  "kernelspec": {
   "display_name": "Python 3.10.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
